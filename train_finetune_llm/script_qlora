#!/bin/bash
#SBATCH --job-name=test_slurm # define job name
#SBATCH --nodes=1             # define node
#SBATCH --gpus-per-node=2    # define gpu limmit in 1 node
#SBATCH --ntasks=1           # define number tasks
#SBATCH --cpus-per-task=24   # There are 24 CPU cores
#SBATCH --nodelist=node002
#SBATCH --time=72:00:00       # Reasonable upper estimate based on previous runs
#SBATCH --partition=defq


# while true; do nvidia-smi > gpu_status.txt; sleep 5; done &
# while true; do nvidia-smi > gpu_status.txt; sleep 5; done &
nvidia-smi
nvidia-smi --query-gpu=timestamp,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 120 >> ${SLURM_JOB_ID}_gpu_usage.log &

echo "-----------------------------"
echo "## Print Python and cuda"

export ACCELERATE_USE_FSDP=1
export FSDP_CPU_RAM_EFFICIENT_LOADING=1


# Execute the Python script using torchrun for distributed training
# accelerate launch --main_process_port 12345 Qlora_finetuning.py  --ddp_find_unused_parameters False || echo "Python script failed with exit code $?"
accelerate launch --main_process_port 29002  Qlora_finetuning.py  --ddp_find_unused_parameters False || echo "Python script failed with exit code $?"


echo "-----------------------------"
echo "SLURM_GPUS_ON_NODE=$SLURM_GPUS_ON_NODE"
echo "SLURM_GPUS_PER_NODE=$SLURM_GPUS_PER_NODE"
echo "SLURM_JOB_GPUS=$SLURM_JOB_GPUS"

echo "-----------------------------"
echo "Exit worker node"
echo "## Print Finish Testing"
# sleep 20
